---
title: "Results"
output:
  html_document:
    toc: true
    toc_float: true
---

<!--- FOR COLUMNS: From http://stackoverflow.com/questions/31753897/2-column-section-in-r-markdown:
Put in your css file or directly in rmarkdown--->
<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
</style>

----

# **Classification**
## Evaluating Performance of Top-3 Performing Transformer Models  

 **Model**                                       | **Accuracy** | **Precision** | **Recall** | **F1** | **Cross-Validation Score** |
|:-----------------------------------------------:|:------------:|:-------------:|:----------:|:------:|:--------------------------:|
| distilbert-base-uncased_SVM_rbf-kernel          | 90.09        | 83.82         | 100        | 91.2   | 85.97   
| distilbert-base-uncased_logistic regression     | 87.39        | 83.08         | 94.74      | 88.52  | 82.47                      |                   |
| google/bigbird-roberta-base_logistic regression | 85.59        | 82.54         | 91.23      | 86.67  | 81.63                      |

<br>

From the table above, we see that the best performing model is the DistilBERT model paired with a Support Vector Machines classifier using an RBF kernel. This model demonstrates the highest performance metrics with a balanced dataset. When cross-validation is used to further test performance with an unbalanced dataset, we find this model also has the best cross-validation score.  

We make use of a balanced dataset of 222 observations in each of our "yes" and "no" target classes for calculating the accuracy, precision, recall, and F-1 score. We split this dataset having 444 total observation into a training set containing 333 observations and a testing set containing 111 observations, respectively (*train-test-split = 75% train - 25% test*). For cross-validating our models, we made use of the entire unbalanced dataset of 600 observations. We effectively had 222 observations for the "yes" class 378 observations for the "no" class.  

<div class="col2">

## **ROC Curves and Confusion Matrices** 

</div>

Distilbert LR

<!-- <img src="images/distilbert_LR_results.png"width="350px"> -->

<div class="col2">


<img src="images/distilbert-base-uncased_lr_classifier_roc_curve.png"width="350px">

<br>
<br>

<img src="images/distilbert-base-uncased_lr_classifier_cf_matrix.png"width="350px">

</div>


Distilbert SVM
 
 <div class="col2">

<!-- <img src="images/distilbert_SVM_results.png"width="350px"> -->

<img src="images/distilbert-base-uncased_rbf-kernel_classifier_roc_curve.png"width="350px">

<br>
<br>


<img src="images/distilbert-base-uncased_rbf-kernel_classifier_cf_matrix.png"width="350px">

</div>

Big Bird Roberta LR

<!-- <img src="images/bigbird_LR_results.png"width="350px"> -->

<div class="col2">

<img src="images/googlebigbird-roberta-base_lr_classifier_roc_curve.png"width="350px">

<br>
<br>


<img src="images/googlebigbird-roberta-base_lr_classifier_cf_matrix.png"width="350px">

</div>

There are some interesting results in our three models' confusion matrices. Firstly, we observe that each model's TN and FP counts are identical. For the TP and FN counts, we observe the following ranking:  

1. DistilBERT-SVM shows best TP-FN tradeoff with 0 FNs,
2. DistilBERT-LR shows next best tradeoff with 3 FNs,
3. Big Bird RoBERTA-LR shows worst performance in terms of TP-FN trade off with 5 FNs.  

<!-- Namely, while we cannot be sure that each model is making the sa predictions on the same articles, the first row of TN’s and FP’s are identical across all models - 43 TN’s and 11 FP’s. -->
Looking at the distilbert SVM confusion matrix, we see improvement from the first distilbert logistic regression confusion matrix, as 3 FN’s move into TP’s.  

And lastly, the Big Bird Roberta Logistic Regression confusion matrix seems to perform worse, compared to the first distilbert logistic regression confusion matrix, as it shows an extra 2 FN’s that seem to move from the TP’s.  

# **Named Entity Recognition**

* dslim results
* expecting that we have some overall accuracy number - compare the # companies we manually labeled to the companies identified by NER
* if we had tagged Apple, MCRSFT, is there a match for those based on NER? 


# 2017 Set Results

## Classification

* classification prediction histogram of pred_prob (highlight green for > 0.5)

## NER

* NER - top 10 innovators
    - can we add %ages to the bars here for innovative mentions/all mentions

<img src="images/2017_top_10_innovative_companies.png"width="500px">

* visualization of co-mentions of innovators


## Conclusions

In conclusion, promising results in terms of leverage state of the art language represetntation models to identify articles relevant to innovation. We have successffuly constructed a pipeline that classifies articles and extracts innovator companies with high accuracy and reasonable confidence. As a result of this work, we/NSF can now...

Drawbacks/Weaknesses: 

* susceptible to bias  e.g., because of large companies with large PR budgets overrepresented in the data
* susceptible to noise e.g., financial news, irrelevant news
* scaling is a challenge - need for labeled data, crossing sectors is difficult, crossing time may be more complex as well

Next steps, to refine this pipeline, extract product names, implement alt approaches e.g. Q&A, perform deduplication on company names