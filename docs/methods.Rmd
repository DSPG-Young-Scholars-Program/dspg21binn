---
title: "Methods"
---



----

<center><img src="images/bert.png" width="500px"></center>  


## BERT

BERT is a pre-trained, bidirectional unsupervised Natural Language Processing model developed by Google in 2018. Trained on the Book Corpus of 2,500 million words and Wikipedia's 800 million words, BERT tokenizes and builds more complex text embeddings that take word context into account, unlike traditional "bag-of-words" approaches. Here, we use BERT in a number of ways: not only as a data transformation tool to convert our text data into text embeddings based on a pretrained model, but also as the basis for named-entity recognition.


### BERT for Data Transformation

#### DistilBERT

DistlBERT is a distilled version of traditional BERT. Uses fewer parameters, which had the interesting effect in our Summer trials of improving accuracy by reducing noise. It also runs faster and is smaller in size.

#### Big Bird

Big Bird is a new State of The Art method of tokenization and language processing. This model emphasizes memory optimization to allow for longer sequences to be processed. This could help us with our current limitation where we can only process the first 512 tokens.

#### Roberta

RoBERTa and DistilRoBERTa – RoBERTa is a most robustly trained version of BERT that also uses much larger datasets to train. (160GB for RoBERTa vs 16GB for BERT). It also uses datasets to train that are more relevant to our applications, such as CC-NEWS (76G), OpenWebText (38G) and Stories (31G) data. DistilRoBERTa is a version of RoBERTa that has been processed using Knowledge Distillation, similar to DistilBERT. Once again, it has a faster performance.

### BERT for Named-Entity Recognition

#### HuggingFace/dslim-bert-base-NER

[Try the demo here!](https://huggingface.co/dslim/bert-base-NER)

----

## Our Approach

<center><img src="images/methods.png" width="800px"></center>  

1. Use BERT to tokenize and build text embeddings for the labeled data sets

2. Use Machine Learning Methods to build and train a model to identify innovative articles using labeled data sets.

3. Use BERT’s Named Entity Recognition methods to extract company names from documents

4. Apply these methods to all articles for a given sector for a given year to extract the count of innovative articles per year, with their associated companies.


----