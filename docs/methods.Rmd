---
title: "Methods"
---

<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
</style>

<div class="col2">
<br>
<br>
</div>

Notes: 

* Explanation of Big-Bird-Roberta as hybrid of the two (combine those sections and add a explanation of combo)
* nothing about machine learning
* NER section - bert base
* separate page for Evaluation


## **Our Approach**  

<center><img src="images/approach_slide.png" width="600px"></center>  

1. Implement various transformer based Natural Language Processing (NLP) models to tokenize and build text embeddings for the labeled data sets,  

2. Leverage supervised Machine Learning methods to train a model using a balanced dataset to classify articles denoting innovation and not denoting innovation,  

3. Evaluate model performance using cross-validation by using the imbalanced dataset,  

4. Apply BERT’s Named Entity Recognition methods to extract company names from classified documents,  

5. Run these methods to all articles for a given sector for a given year to extract the count of innovative articles per year, with their associated companies.  

----

## **Transformer Based Natural Language Processing Models**  

<center><img src="images/bert.png" width="500px"></center>  

### What is BERT?

BERT is a pre-trained, bidirectional unsupervised Natural Language Processing model developed by Google in 2018. Trained on the Book Corpus of 2,500 million words and Wikipedia's 800 million words, BERT tokenizes and builds more complex text embeddings that take word context into account, unlike traditional "bag-of-words" approaches. Here, we use BERT in several ways: not only as a data transformation tool to convert our text data into text embeddings based on a pre-trained model for classification but also as the basis for named-entity recognition. 

Try the [demo here!](https://huggingface.co/dslim/bert-base-NER)

### How do we make use of BERT for our work?

We make use of the aforementioned models and their variations provided by Huggingface for 2 processes (see Appendix for technical details).

* *Data Transformation:* We pre-process our documents, such that the text of our news articles are represented by new word-embeddings using these state-of-the-art NLP models. We use DistilBERT and a hybrid Big Bird-RoBERTa model for data transformation here. Once the articles are transformed, we then leverage classic supervised machine learning techniques in order to classify the articles based on likelihood of describing innovation. These include:
    - *Logistic Regression*
    - *Support Vector Machines*
* *Named Entity Recognition:* Once a set of innovation-likely articles are identified, we make use of the BERT base model for Named Entity Recogition.

----

### BERT Models  

There are several different BERT-like language representation models, usually designed to suit a specific task or optimized for a specific type of data. These include:

<br>

#### DistilBERT

<div class="col2">
DistlBERT is a distilled version of traditional BERT. Uses fewer parameters, which had an interesting effect in our Summer trials of improving accuracy by reducing noise. It also runs faster and is smaller in size.   
<br>
<br>
<center><img src="images/distilbert.png" width="300px"></center>  
</div>

#### Big Bird  

<div class="col2">
Big Bird is a new State of The Art method of tokenization and language processing. This model emphasizes memory optimization to allow for longer sequences to be processed. This could help us with our current limitation where we can only process the first 512 tokens. 
<br>
<br>
<!-- <center><img src="images/bigbird.jpeg" width="500px"></center>   -->
<center><img src="images/bigbirdgooglelogo.png" width="200px"></center>  
</div>
<br>

#### Roberta  

<div class="col2">

RoBERTa and DistilRoBERTa – RoBERTa is a most robustly trained version of BERT that also uses much larger datasets to train. (160GB for RoBERTa vs 16GB for BERT). It also uses datasets to train that are more relevant to our applications, such as CC-NEWS (76G), OpenWebText (38G), and Stories (31G) data. DistilRoBERTa is a version of RoBERTa that has been processed using Knowledge Distillation, similar to DistilBERT. Once again, it has faster performance. 
<br>
<br>
<br>
<!-- <center><img src="images/bigbird.jpeg" width="500px"></center>   -->
<center><img src="images/roberta.png" width="300px"></center>  
<br>
</div>
<br>

----

### Cross-Validation  

Cross-validation is a statistical technique used to evaluate the performance of a machine learning model without splitting our data for training and testing. This summer, we used a method of cross-validation known as “k-fold” to evaluate the skill of our classifier models, where k=10. In 10-fold cross-validation, a given data set is split into 10 sections where each section/fold is used as a testing set at some point. In the first iteration, the first fold is used to test the model and the rest are used to train the model. In the second iteration, 2nd fold is used as the testing set while the rest serve as the training set. This process is repeated until each fold of the 10 folds has been used as the testing set. The average score is then calculated for the cross-validation score.  

<center><img src="images/crossvalidation.png" width="900px"></center>  

----