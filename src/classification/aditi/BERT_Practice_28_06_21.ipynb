{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-SXM2-32GB'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "torch.cuda.current_device()\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  a stirring , funny and finally transporting re...  1\n",
       "1  apparently reassembled from the cutting room f...  0\n",
       "2  they presume their audience wo n't sit still f...  0\n",
       "3  this is a visually stunning rumination on love...  1\n",
       "4  jonathan parker 's bartleby should have been t...  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model/tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = transformers.AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'Slim', 'score': 0.9975119829177856, 'entity': 'B-PER', 'index': 4, 'start': 11, 'end': 15}, {'word': 'S', 'score': 0.9924010038375854, 'entity': 'I-PER', 'index': 5, 'start': 16, 'end': 17}, {'word': '##hady', 'score': 0.9969489574432373, 'entity': 'I-PER', 'index': 6, 'start': 17, 'end': 21}]\n"
     ]
    }
   ],
   "source": [
    "list_of_models = [\"dslim/bert-base-NER\"]\n",
    "\n",
    "for models in list_of_models:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(models)\n",
    "    model = transformers.AutoModelForTokenClassification.from_pretrained(models)\n",
    "    nlp = transformers.pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "    example = \"My name is Slim Shady\"\n",
    "\n",
    "    ner_results = nlp(example)\n",
    "    print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 73)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[ 8.7263e+00, -2.2096e-01, -9.2287e-01,  ..., -1.5831e+00,\n",
       "          -1.3599e+00, -1.2300e+00],\n",
       "         [ 1.0925e+01, -3.4858e-01, -1.7687e+00,  ..., -1.8523e+00,\n",
       "          -1.2979e+00, -1.5752e+00],\n",
       "         [ 1.1218e+01, -4.8180e-01, -1.4845e+00,  ..., -1.6833e+00,\n",
       "          -1.6667e+00, -1.2625e+00],\n",
       "         ...,\n",
       "         [ 9.7625e+00, -5.5739e-01, -1.5718e+00,  ..., -1.6915e+00,\n",
       "          -1.3323e+00, -1.5667e+00],\n",
       "         [ 9.5080e+00, -5.0637e-01, -1.3887e+00,  ..., -1.5824e+00,\n",
       "          -1.3212e+00, -1.6749e+00],\n",
       "         [ 9.1156e+00, -4.1663e-01, -1.4644e+00,  ..., -1.5645e+00,\n",
       "          -1.3705e+00, -1.6793e+00]],\n",
       "\n",
       "        [[ 9.0401e+00, -6.3305e-01, -8.1791e-01,  ..., -1.3848e+00,\n",
       "          -1.5682e+00, -1.3040e+00],\n",
       "         [ 1.1295e+01, -9.9236e-01, -1.3954e+00,  ..., -1.5412e+00,\n",
       "          -1.6942e+00, -1.2654e+00],\n",
       "         [ 1.1355e+01, -1.1130e+00, -1.1480e+00,  ..., -1.3878e+00,\n",
       "          -1.8952e+00, -1.2091e+00],\n",
       "         ...,\n",
       "         [ 1.0374e+01, -1.0483e+00, -1.5424e+00,  ..., -1.2891e+00,\n",
       "          -1.5144e+00, -1.3883e+00],\n",
       "         [ 9.6637e+00, -9.5458e-01, -1.6482e+00,  ..., -1.2137e+00,\n",
       "          -1.3676e+00, -1.4530e+00],\n",
       "         [ 6.8029e+00, -6.6357e-01, -1.6711e+00,  ..., -1.1769e+00,\n",
       "          -9.1431e-01, -1.5622e+00]],\n",
       "\n",
       "        [[ 9.3273e+00, -3.6916e-01, -1.2702e+00,  ..., -1.6868e+00,\n",
       "          -1.5548e+00, -1.2353e+00],\n",
       "         [ 1.1228e+01, -8.2948e-01, -1.5734e+00,  ..., -1.4911e+00,\n",
       "          -1.6521e+00, -1.2652e+00],\n",
       "         [ 1.1324e+01, -9.2376e-01, -1.6259e+00,  ..., -1.4567e+00,\n",
       "          -1.8791e+00, -1.2819e+00],\n",
       "         ...,\n",
       "         [ 1.0657e+01, -8.7086e-01, -1.6911e+00,  ..., -1.5801e+00,\n",
       "          -1.6786e+00, -1.5855e+00],\n",
       "         [ 1.0647e+01, -9.1909e-01, -1.6218e+00,  ..., -1.5589e+00,\n",
       "          -1.7633e+00, -1.5631e+00],\n",
       "         [ 1.0728e+01, -1.0442e+00, -1.4325e+00,  ..., -1.4141e+00,\n",
       "          -1.8930e+00, -1.4053e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 9.1714e+00, -2.8756e-01, -1.3001e+00,  ..., -1.6960e+00,\n",
       "          -1.5261e+00, -1.2784e+00],\n",
       "         [ 1.1215e+01, -5.9871e-01, -1.6570e+00,  ..., -1.7292e+00,\n",
       "          -1.4854e+00, -1.3761e+00],\n",
       "         [ 1.1152e+01, -6.2499e-01, -1.3024e+00,  ..., -1.6430e+00,\n",
       "          -1.7260e+00, -1.3377e+00],\n",
       "         ...,\n",
       "         [ 9.6934e+00, -7.8614e-01, -1.3529e+00,  ..., -1.2704e+00,\n",
       "          -1.6656e+00, -1.2732e+00],\n",
       "         [ 1.0062e+01, -9.4513e-01, -1.1440e+00,  ..., -1.2761e+00,\n",
       "          -1.6458e+00, -1.4674e+00],\n",
       "         [ 1.0488e+01, -9.5379e-01, -1.4232e+00,  ..., -1.4013e+00,\n",
       "          -1.6232e+00, -1.4474e+00]],\n",
       "\n",
       "        [[ 1.0098e+01, -5.3727e-01, -1.4128e+00,  ..., -1.7294e+00,\n",
       "          -1.5974e+00, -1.4347e+00],\n",
       "         [ 1.0806e+01, -3.8005e-01, -2.1102e+00,  ..., -2.0107e+00,\n",
       "          -1.1221e+00, -1.8445e+00],\n",
       "         [ 1.1245e+01, -6.8860e-01, -1.7091e+00,  ..., -1.5560e+00,\n",
       "          -1.7706e+00, -1.3365e+00],\n",
       "         ...,\n",
       "         [ 1.0237e+01, -8.2495e-01, -1.8972e+00,  ..., -1.6545e+00,\n",
       "          -1.4202e+00, -1.6768e+00],\n",
       "         [ 1.0096e+01, -8.3728e-01, -1.8705e+00,  ..., -1.6518e+00,\n",
       "          -1.5509e+00, -1.6805e+00],\n",
       "         [ 1.0562e+01, -9.5786e-01, -1.3161e+00,  ..., -1.4482e+00,\n",
       "          -1.7852e+00, -1.5519e+00]],\n",
       "\n",
       "        [[ 8.6922e+00, -2.0382e-01, -8.1304e-01,  ..., -1.5275e+00,\n",
       "          -1.3245e+00, -1.1191e+00],\n",
       "         [ 1.0506e+01, -8.0133e-04, -1.6832e+00,  ..., -2.0231e+00,\n",
       "          -1.2350e+00, -1.6461e+00],\n",
       "         [ 1.1154e+01, -1.6744e-01, -1.3236e+00,  ..., -1.6528e+00,\n",
       "          -1.8155e+00, -1.1888e+00],\n",
       "         ...,\n",
       "         [ 9.3495e+00,  1.4362e-01, -4.6537e-01,  ..., -1.8920e+00,\n",
       "          -2.0127e+00, -1.6259e+00],\n",
       "         [ 8.8363e+00, -1.7060e-01, -8.9630e-01,  ..., -1.6125e+00,\n",
       "          -1.4663e+00, -1.5707e+00],\n",
       "         [ 8.8610e+00, -5.7696e-01, -1.1956e+00,  ..., -1.6164e+00,\n",
       "          -1.1874e+00, -1.6008e+00]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-AM_BERT]",
   "language": "python",
   "name": "conda-env-.conda-AM_BERT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
